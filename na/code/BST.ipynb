{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'field_dims', 'users', 'books', 'sub', 'idx2user', 'idx2isbn', 'user2idx', 'isbn2idx', 'X_train', 'X_valid', 'y_train', 'y_valid', 'train_dataloader', 'valid_dataloader', 'test_dataloader'])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data 로드\n",
    "with open('/opt/ml/data.pickle', 'rb') as fr:\n",
    "    data = pickle.load(fr)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 68069, 149570,      6,   3865,  11571,     27,  62059],\n",
       "      dtype=uint32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'] = data['train'].drop([\"user_mean\",\"book_mean\"],axis=1)\n",
    "data['test'] = data['test'].drop([\"user_mean\",\"book_mean\"],axis=1)\n",
    "data[\"field_dims\"] = data[\"field_dims\"][:7] \n",
    "data[\"field_dims\"]\n",
    "# data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#                                                     data['train'].drop(['rating'], axis=1),\n",
    "#                                                     data['train']['rating'],\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     random_state=42,\n",
    "#                                                     shuffle=True\n",
    "#                                                     )\n",
    "# data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "train_dataset = TensorDataset(torch.LongTensor(data['X_train'].values), torch.LongTensor(data['y_train'].values))\n",
    "valid_dataset = TensorDataset(torch.LongTensor(data['X_valid'].values), torch.LongTensor(data['y_valid'].values))\n",
    "test_dataset = TensorDataset(torch.LongTensor(data['test'].values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    " \n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,field_dims: np.ndarray,d_feat=128,n_head=5,\n",
    "                 actv=F.relu,USE_BIAS=True,dropout=0.1,device=None):\n",
    "        \"\"\"\n",
    "        :param d_feat: feature dimension\n",
    "        :param n_head: number of heads\n",
    "        :param actv: activation after each linear layer\n",
    "        :param USE_BIAS: whether to use bias\n",
    "        :param dropout_p: dropout rate\n",
    "        :device: which device to use (e.g., cuda:0)\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        if (d_feat%n_head) != 0:\n",
    "            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \n",
    "        self.d_feat = d_feat\n",
    "        self.n_head = n_head\n",
    "        self.d_head = self.d_feat // self.n_head\n",
    "        self.actv = actv\n",
    "        self.USE_BIAS = USE_BIAS\n",
    "        self.dropout = dropout # prob. of zeroed\n",
    "        self.embdding = FeaturesEmbedding(field_dims, self.d_feat)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    def forward(self,X,mask=None):\n",
    "        \"\"\"\n",
    "        :param Q: [n_batch, n_Q, d_feat]\n",
    "        :param K: [n_batch, n_K, d_feat]\n",
    "        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n",
    "        :param mask: \n",
    "        \"\"\"\n",
    "        Q = self.embdding(X)\n",
    "        K = self.embdding(X)\n",
    "        V = self.embdding(X)\n",
    "\n",
    "        n_batch = Q.shape[0]\n",
    "        Q_feat = self.lin_Q(Q) \n",
    "        K_feat = self.lin_K(K) \n",
    "        V_feat = self.lin_V(V)\n",
    "        # Q_feat: [n_batch, n_Q, d_feat]\n",
    "        # K_feat: [n_batch, n_K, d_feat]\n",
    "        # V_feat: [n_batch, n_V, d_feat]\n",
    "\n",
    "        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n",
    "        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        # Q_split: [n_batch, n_head, n_Q, d_head]\n",
    "        # K_split: [n_batch, n_head, n_K, d_head]\n",
    "        # V_split: [n_batch, n_head, n_V, d_head]\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        d_K = K.size()[-1] # key dimension\n",
    "        scores = torch.matmul(Q_split,K_split.permute(0,1,3,2)) / np.sqrt(d_K)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0,-1e9)\n",
    "        attention = torch.softmax(scores,dim=-1)\n",
    "        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\n",
    "        # attention: [n_batch, n_head, n_Q, n_K]\n",
    "        # x_raw: [n_batch, n_head, n_Q, d_head]\n",
    "\n",
    "        # Reshape x\n",
    "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n",
    "        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n",
    "        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n",
    "        # x_rsh2: [n_batch, n_Q, d_feat]\n",
    "\n",
    "        # Linear\n",
    "        x = self.lin_O(x_rsh2)\n",
    "        # x: [n_batch, n_Q, d_feat]\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, n_head, input_dim, embed_dim, embed_dims, USE_BIAS, dropout, device, output_layer=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(field_dims = field_dims,\n",
    "                                            d_feat=embed_dim,n_head=n_head,\n",
    "                                            actv=F.relu,USE_BIAS=True,dropout=0.1)\n",
    "        layers = list()\n",
    "        d_feat=embed_dim\n",
    "        self.input_dim = input_dim\n",
    "        # self.embdding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.LeakyReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        # if output_layer:\n",
    "        #     layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "        self.fc = torch.nn.Linear(embed_dims[-1] + d_feat, 1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        # x = torch.tensor(x,dtype= np.long)\n",
    "        # x_user = torch.cat((x[:,0].unsqueeze(1),x[:,2].unsqueeze(1)),dim=1)\n",
    "        # x_item = torch.cat((x[:,1].unsqueeze(1),x[:,3:]),dim=1)\n",
    "        # x_user = torch.tensor(x_user,dtype= np.long)\n",
    "        # x_item = torch.tensor(x_item,dtype= np.long)\n",
    "        # x_user = self.embdding(x_user)\n",
    "        x = self.attention(x)\n",
    "        print(x)\n",
    "        # print(x_user)\n",
    "        # print(x_item)\n",
    "        user_x = x[:, np.array((0, ), dtype=np.long)].squeeze(1)\n",
    "        item_x = x[:, np.array((1, ), dtype=np.long)].squeeze(1)\n",
    "        gmf = user_x * item_x\n",
    "        # x = torch.cat((x_user[:,0],x_item[:,0],x_user[:,1],x_item[:,1:]),dim=1)\n",
    "        x = self.mlp(x.view(-1, self.input_dim))\n",
    "\n",
    "        x = torch.cat([gmf, x], dim=1)\n",
    "\n",
    "        x = self.fc(x).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_dims = data['train'].drop([\"rating\"],axis=1).nunique().values\n",
    "# field_dims = np.array([field_dims[1]]+list(field_dims[3:]))\n",
    "# offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "# x = torch.tensor(data['train'].drop([\"rating\"],axis=1).values)\n",
    "# x = torch.tensor(x,dtype= np.long )\n",
    "# x = torch.cat((x[:,1].unsqueeze(1),x[:,3:]),dim=1)\n",
    "# print(x)\n",
    "# f = FeaturesEmbedding(field_dims, 10)\n",
    "# f(x)\n",
    "\n",
    "# next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(real: list, predict: list) -> float:\n",
    "    pred = np.array(predict)\n",
    "    return np.sqrt(np.mean((real-pred) ** 2))\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class BST:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = RMSELoss()\n",
    "\n",
    "        self.train_dataloader = data['train_dataloader']\n",
    "        self.valid_dataloader = data['valid_dataloader']\n",
    "        self.field_dims = data['field_dims']\n",
    "        self.n_head  = 10\n",
    "        self.embed_dim = 60\n",
    "        self.epochs = 5 #args.EPOCHS\n",
    "        self.learning_rate = 5e-3 #args.LR\n",
    "        self.weight_decay = 1e-5 #args.WEIGHT_DECAY\n",
    "        self.log_interval = 100\n",
    "\n",
    "        self.device = \"cuda\"#args.DEVICE\n",
    "\n",
    "        self.mlp_dims = [1024,512,256]#args.NCF_MLP_DIMS\n",
    "        self.dropout = 0.1 #args.NCF_DROPOUT\n",
    "\n",
    "        self.model = MultiLayerPerceptron(field_dims=self.field_dims, n_head = self.n_head, \n",
    "                                          input_dim = len(self.field_dims) * self.embed_dim, embed_dim = self.embed_dim, embed_dims = self.mlp_dims,\n",
    "                                        USE_BIAS=True,dropout=0.2,device=self.device, output_layer=True).to(self.device)\n",
    "        \n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=self.learning_rate, \n",
    "                                          amsgrad=True, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "      # model: type, optimizer: torch.optim, train_dataloader: DataLoader, criterion: torch.nn, device: str, log_interval: int=100\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            tk0 = tqdm.tqdm(self.train_dataloader, smoothing=0, mininterval=1.0)\n",
    "            for i, (fields, target) in enumerate(tk0):\n",
    "                fields, target = fields.to(self.device), target.to(self.device)\n",
    "                y = self.model(fields)\n",
    "                loss = self.criterion(y, target.float())\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                if (i + 1) % self.log_interval == 0:\n",
    "                    tk0.set_postfix(loss=total_loss / self.log_interval)\n",
    "                    total_loss = 0\n",
    "\n",
    "            rmse_score = self.predict_train()\n",
    "            print('epoch:', epoch, 'validation: rmse:', rmse_score)\n",
    "\n",
    "\n",
    "    def predict_train(self):\n",
    "        self.model.eval()\n",
    "        targets, predicts = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for fields, target in tqdm.tqdm(self.valid_dataloader, smoothing=0, mininterval=1.0):\n",
    "                fields, target = fields.to(self.device), target.to(self.device)\n",
    "                y = self.model(fields)\n",
    "                targets.extend(target.tolist())\n",
    "                predicts.extend(y.tolist())\n",
    "        return rmse(targets, predicts)\n",
    "\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        self.model.eval()\n",
    "        predicts = list()\n",
    "        with torch.no_grad():\n",
    "            for fields in tqdm.tqdm(dataloader, smoothing=0, mininterval=1.0):\n",
    "                fields = fields[0].to(self.device)\n",
    "                y = self.model(fields)\n",
    "                predicts.extend(y.tolist())\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125771/3658618194.py:6: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [274], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m BST(data)\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn [273], line 38\u001b[0m, in \u001b[0;36mBST.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_dims \u001b[39m=\u001b[39m [\u001b[39m1024\u001b[39m,\u001b[39m512\u001b[39m,\u001b[39m256\u001b[39m]\u001b[39m#args.NCF_MLP_DIMS\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m#args.NCF_DROPOUT\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m MultiLayerPerceptron(field_dims\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfield_dims, n_head \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_head, \n\u001b[1;32m     39\u001b[0m                                   input_dim \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfield_dims) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, embed_dim \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, embed_dims \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_dims,\n\u001b[1;32m     40\u001b[0m                                 USE_BIAS\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,dropout\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, output_layer\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate, \n\u001b[1;32m     44\u001b[0m                                   amsgrad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, weight_decay\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_decay)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model = BST(data)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
